{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5995808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple | to | build | a | Hong | Kong | factory | for | $ | 6 | milion | \n",
      "--------\n",
      "Apple - ORG - Companies, agencies, institutions, etc.\n",
      "Hong Kong - GPE - Countries, cities, states\n",
      "6 - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc8 = nlp(u\"Apple to build a Hong Kong factory for $6 milion\")\n",
    "for token in doc8:\n",
    "    print(token.text,end=\" | \")\n",
    "print('\\n--------')\n",
    "for ent in doc8.ents:\n",
    "    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddbf785",
   "metadata": {},
   "source": [
    "Noun chunks similar to Doc.ents, Doc.noun_chunks are another objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed1343cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars shifts insurance liability\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "doc9 = nlp(u\"Autonomous cars shifts insurance liability towards manufacturers.\")\n",
    "for chunk in doc9.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a495fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red cars\n",
      "higher insurance rates\n"
     ]
    }
   ],
   "source": [
    "doc10 = nlp(u\"Red cars do not carry higher insurance rates.\")\n",
    "for chunk in doc10.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80212f63",
   "metadata": {},
   "source": [
    "Stemming is a somewhat crude method for cataloging related works; it essentially chops off letters from the end until the stem is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4769a16",
   "metadata": {},
   "source": [
    "#### Portar Stemmer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1242f4",
   "metadata": {},
   "source": [
    "Portar Stemmer One of the most common and effective - stemming tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc23545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the toolkit and the full portar stemmer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0449e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run--->run\n",
      "runner--->runner\n",
      "running--->run\n",
      "ran--->ran\n",
      "runs--->run\n",
      "easily--->easili\n",
      "fairly--->fairli\n"
     ]
    }
   ],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "for word in words :\n",
    "    print(word+'--->'+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02482ebb",
   "metadata": {},
   "source": [
    "Note how the stemmer recognize \"runner\" as a noun , not a verb from or participate.Also,the adverbs \"easily\" and \"fairly\" are stemmed to the unusual root \"easili\" and \"fairli\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f35a668",
   "metadata": {},
   "source": [
    "#### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98918695",
   "metadata": {},
   "source": [
    "Snowball stemmer it offers a slight improvement over the original portar stemmer, both in logic and speed. Since nltk uses the name Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eabce8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#the SnowBall stemmer requires that you pass a lanuage parameter\n",
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a50a4f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run--->run\n",
      "runner--->runner\n",
      "running--->run\n",
      "ran--->ran\n",
      "runs--->run\n",
      "easily--->easili\n",
      "fairly--->fairli\n"
     ]
    }
   ],
   "source": [
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "# words = ['generous','generation','generously','generate']\n",
    "for word in words :\n",
    "    print(word+'--->'+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e41ac2f",
   "metadata": {},
   "source": [
    "Stemming has its drwbacks. If given the token saw, stemming might always \n",
    "Lemmatization would likely return either see or saw depending on whether "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84f3787b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I----->i\n",
      "am----->am\n",
      "meeting----->meet\n",
      "him----->him\n",
      "tommorow----->tommorow\n",
      "at----->at\n",
      "the----->the\n",
      "meeting----->meet\n"
     ]
    }
   ],
   "source": [
    "phrase = 'I am meeting him tommorow at the meeting'\n",
    "for word in phrase.split():\n",
    "    print(word+'----->'+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a12c65",
   "metadata": {},
   "source": [
    "Here \"meeting\" appears twice once for verb and another is for noun, and yet the stemmer treats both equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f9db9b",
   "metadata": {},
   "source": [
    "##### Lemmatization in contrast to stemming , lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a morphological analysis to words. The lemma of \"was\" is \"be\" and lemma of \"mice\" is \"mouse\". Further the lemma of \"meeting\" might be \"meet\" or \"meeting\" depending on its uses in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "272c3f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "160ead1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON\t 4690420944186131903 \t I\n",
      "am \t AUX\t 10382539506755952630 \t be\n",
      "a \t DET\t 11901859001352538922 \t a\n",
      "runner \t NOUN\t 12640964157389618806 \t runner\n",
      "running \t VERB\t 12767647472892411841 \t run\n",
      "in \t ADP\t 3002984154512732771 \t in\n",
      "a \t DET\t 11901859001352538922 \t a\n",
      "race \t NOUN\t 8048469955494714898 \t race\n",
      "becuase \t NOUN\t 3636336227294319702 \t becuase\n",
      "I \t PRON\t 4690420944186131903 \t I\n",
      "love \t VERB\t 3702023516439754181 \t love\n",
      "to \t PART\t 3791531372978436496 \t to\n",
      "run \t VERB\t 12767647472892411841 \t run\n",
      "since \t SCONJ\t 10066841407251338481 \t since\n",
      "I \t PRON\t 4690420944186131903 \t I\n",
      "ran \t VERB\t 12767647472892411841 \t run\n",
      "today \t NOUN\t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u\"I am a runner running in a race becuase I love to run since I ran today\")\n",
    "for token in doc1:\n",
    "    print(token.text,'\\t',token.pos_+'\\t',token.lemma,'\\t',token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1309d4d1",
   "metadata": {},
   "source": [
    "#token.pos_ :- part of speech\n",
    "#token.lemma_ :- meaning of lemma\n",
    "Here we're using an f-string to fromat the printed text by setting ,minimum field widths and adding a left-align to the lemma hash value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3839410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemma(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9df4a33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I\n",
      "saw          VERB   11925638236994514241   see\n",
      "eighteen     NUM    9609336664675087640    eighteen\n",
      "mice         NOUN   1384165645700560590    mouse\n",
      "today        NOUN   11042482332948150395   today\n",
      "!            PUNCT  17494803046312582752   !\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"I saw eighteen mice today!\")\n",
    "show_lemma(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7d91934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I\n",
      "am           AUX    10382539506755952630   be\n",
      "meeting      VERB   6880656908171229526    meet\n",
      "him          PRON   1655312771067108281    he\n",
      "tommorow     VERB   14881451523362505806   tommorow\n",
      "at           ADP    11667289587015813222   at\n",
      "the          DET    7425985699627899538    the\n",
      "meeting      NOUN   14798207169164081740   meeting\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u\"I am meeting him tommorow at the meeting\")\n",
    "show_lemma(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a53c6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That         PRON   4380130941430378203    that\n",
      "'s           AUX    10382539506755952630   be\n",
      "an           DET    15099054000809333061   an\n",
      "enormous     ADJ    17917224542039855524   enormous\n",
      "automobile   NOUN   7211811266693931283    automobile\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u\"That's an enormous automobile\")\n",
    "show_lemma(doc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffcdcb1",
   "metadata": {},
   "source": [
    "Note that lemmatization does not reduce words to their most basic synonym - that is, enormous doesn't become big and automobile doesn't become car."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c2eb7",
   "metadata": {},
   "source": [
    "#### StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7a16a6",
   "metadata": {},
   "source": [
    "Stopwords words like \"a\" and \"the\" appears so frequently that they don't require tagging as thoroughly as nouns. Spacy holds 326 english stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03867378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'re', 'now', 'yourself', 'then', 'since', 'if', 'has', 'four', 'per', 'will', 'beforehand', 'hence', 'within', 'whom', 'seemed', 'among', 'twenty', 'much', 'they', 'behind', 'keep', 'us', 'between', 'might', 'it', 'last', 'very', 'twelve', 'because', 'throughout', 'everything', 'wherever', 'his', 'can', 'nothing', 'none', 'several', 'its', 'although', 'here', 'unless', 'down', 'ever', 'quite', 'a', 'hereafter', 'only', 'five', 'both', 'sometime', 'doing', 'nobody', 'besides', '‘ve', 'elsewhere', 'therein', 'our', 'whereby', 'your', 'next', 'above', 'whoever', 'name', 'been', 'really', 'somehow', 'is', 'still', 'at', 'thus', 'used', 'of', 'across', 'thereafter', 'nowhere', '’ve', 'should', 'about', 'former', 'by', 'no', 'even', 'before', 'made', '‘s', 'we', '‘m', 'were', 'themselves', 'after', 'whither', 'may', 'due', 'towards', 'from', 'hundred', 'for', 'more', 'show', 'again', 'serious', 'ourselves', 'formerly', 'as', 'whereupon', 'along', 'amount', 'beyond', 'through', 'beside', 'bottom', 'and', 'she', 'herein', 'nor', 'neither', 'these', 'amongst', 'in', 'else', 'just', 'ours', 'are', 'fifty', 'those', 'that', 'himself', 'afterwards', 'do', \"'re\", 'than', 'three', 'never', 'own', 'whole', 'take', 'regarding', 'them', 'latterly', 'over', 'whenever', 'once', 'already', 'most', 'well', 'nevertheless', 'their', 'hers', 'whereafter', 'he', 'toward', 'get', 'am', 'this', 'him', 'whose', 'everyone', 'enough', 'to', 'where', 'could', 'rather', 'least', 'how', 'under', 'always', 'also', 'her', 'i', 'whereas', 'ten', 'when', 'call', 'during', 'without', '‘d', 'though', 'off', 'anyone', 'would', 'such', 'becomes', 'whether', 'few', 'meanwhile', 'which', 'into', 'why', 'third', 'either', 'latter', 'some', \"'ve\", 'seeming', 'against', 'together', 'herself', 'too', 'others', 'you', 'below', 'using', 'say', 'empty', \"'m\", 'anyhow', 'put', 'around', 'there', 'yet', 'done', 'namely', 'until', \"n't\", 'become', 'somewhere', 'many', '’d', 'n’t', 'becoming', 'but', 'first', \"'ll\", 'mostly', 'itself', 'out', 'give', 'top', 'had', 'anything', \"'s\", 'upon', 'six', 'ca', 'forty', 'same', 'who', '’re', 'n‘t', 'anyway', 'thence', 'therefore', 'does', '’ll', \"'d\", 'while', 'onto', 'other', 'further', 'whence', 'someone', 'did', 'however', 'have', 'whatever', '’s', 'be', 'seem', 'seems', 'anywhere', 'sixty', 'what', 'via', 'go', 'fifteen', 'move', 'please', 'or', 'any', 'was', 'full', 'thereupon', 'so', 'various', 'all', 'on', '‘ll', 'everywhere', 'thru', 'something', 'thereby', 'every', 'myself', 'with', 'less', 'almost', 'back', 'must', 'yours', 'up', 'my', 'eight', 'wherein', 'alone', 'moreover', 'being', 'hereupon', 'part', 'eleven', 'one', 'nine', 'yourselves', 'front', 'indeed', 'mine', 'each', 'noone', 'hereby', 'often', 'perhaps', 'another', 'except', 'became', 'side', 'an', 'not', 'two', '‘re', 'me', 'see', 'make', '’m', 'the', 'sometimes', 'cannot', 'otherwise'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "print(nlp.Defaults.stop_words)\n",
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3e103c",
   "metadata": {},
   "source": [
    "#### To see if a word is a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b64a8b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['myself'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bd604be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['mystery'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b85967",
   "metadata": {},
   "source": [
    "#### To add a stp word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c325f8c5",
   "metadata": {},
   "source": [
    "Here we're trying to add one new stop word i.e. btw into the existing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a852f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the word to the set of stp words. Use Lowercase!\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "\n",
    "#Set the stop words tag on the lexeme \n",
    "nlp.vocab['btw'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6b98ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d48b930",
   "metadata": {},
   "source": [
    "#### To Remove a Stop Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0ea1e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the word from the set of stop words\n",
    "nlp.Defaults.stop_words.remove('beyond')\n",
    "\n",
    "#REmove the stop_word tag from the lexeme\n",
    "nlp.vocab['beyond'].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4cf98a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)\n",
    "# Here, we remove one 'beyond' stop_words from existing stop_words and assign it as is_stop :- False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67281930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
